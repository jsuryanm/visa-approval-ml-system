# visa-approval-ml-system
An End-to-End machine learning system for US visa approval prediction, using automated data pipelines, model training and evaluation, FastAPI-based inference.

## Project Overview

This repository implements a **production-style ML system** that covers the complete lifecycle of a machine learning model:

- Data ingestion from MongoDB
- Data validation and drift detection using Evidently.ai
- Feature engineering and preprocessing
- Model training with hyperparameter tuning
- Model evaluation against a production model
- Model registry using AWS S3
- Inference via FastAPI
- UI for predictions
- Containerized deployment on AWS EC2
- CI/CD using GitHub Actions

---

## Problem Statement

Given historical US visa application data, predict whether a visa application will be:

- **Approved**
- **Rejected**

Prediction is done using features such as:
- Applicants level of education
- Continent 
- Region of employment
- Salary
- Applicant work experience
- Unit of wage (Hourly, Weekly, Monthly and Yearly)
- Requires any job training.
- Number of employees in company
- Number of years company has been in operation

Using the KNN Classifier or Random Forest Classifier (the model that is used for inferencing is the one that has higher cross validation f1 score higher than the benchmarked score).  

---
##  System Architecture

```
MongoDB
  â†“
Data Ingestion
  â†“
Data Validation (Schema + Drift Detection)
  â†“
Data Transformation (Feature Engineering + Encoding)
  â†“
Model Training (GridSearchCV)
  â†“
Model Evaluation (Compare with Production Model)
  â†“
Model Pusher
  â†“
AWS S3 (Model Registry)
  â†“
FastAPI Inference Service
  â†“
UI (HTML / Streamlit)
```

---

## Project Directory Structure
```
visa-approval-ml-system/
â”œâ”€â”€ flowcharts/ # contains flowchart diagrams for the workflow of the pipelines
â”‚ â”œâ”€â”€ data_ingestion.png
â”‚ â”œâ”€â”€ data_validation.png
â”‚ â”œâ”€â”€ data_transformation.png
â”‚ â”‚â”€â”€ model_trainer.png
â”‚ â”œâ”€â”€ model_evaluation.png
â”‚ â””â”€â”€ model_pusher.png
â”œâ”€â”€ us_visa/
â”‚ â”œâ”€â”€ components/ # Core pipeline stages
â”‚ â”‚ â”œâ”€â”€ data_ingestion.py
â”‚ â”‚ â”œâ”€â”€ data_validation.py
â”‚ â”‚ â”œâ”€â”€ data_transformation.py
â”‚ â”‚ â”œâ”€â”€ model_trainer.py
â”‚ â”‚ â”œâ”€â”€ model_evaluation.py
â”‚ â”‚ â””â”€â”€ model_pusher.py
â”‚ â”‚
â”‚ â”œâ”€â”€ pipeline/
â”‚ â”‚ â”œâ”€â”€ training_pipeline.py
â”‚ â”‚ â””â”€â”€ prediction_pipeline.py
â”‚ â”‚
â”‚ â”œâ”€â”€ entity/ # Config & artifact schemas
â”‚ â”‚ â”œâ”€â”€ config_entity.py
â”‚ â”‚ â”œâ”€â”€ artifact_entity.py
â”‚ â”‚ â””â”€â”€ estimator.py
â”‚ â”‚
â”‚ â”œâ”€â”€ cloud_storage/
â”‚ â”‚ â”œâ”€â”€ aws_connection.py
â”‚ â”‚ â””â”€â”€ aws_storage.py
â”‚ â”‚
â”‚ â”œâ”€â”€ constants/
â”‚ â”‚ â””â”€â”€ constant.py
â”‚ â”‚
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â””â”€â”€ main_utils.py
â”‚ â”‚
â”‚ â”œâ”€â”€ logger/
â”‚ â”‚ â””â”€â”€ logger.py
â”‚ â”‚
â”‚ â””â”€â”€ exception/
â”‚ â””â”€â”€ exceptions.py
â”‚
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ usvisa.html # UI template
â”‚
â”œâ”€â”€ static/
â”‚ â””â”€â”€ css/styles.css
â”‚ 
â”œâ”€â”€ app.py # FastAPI entrypoint
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

### The workflow process behind the pipelines 
- The project follows a **configuration-driven, modular pipeline architecture** suitable for production ML systems.
- Each pipeline stage is **independent, testable, and reusable**, with clear separation of configuration, logic, and outputs.

### Constants Layer (`constants/constant.py`)
- Centralized definition of:
  - Pipeline names and artifact directories
  - File and folder naming conventions
  - Target column
  - Model performance thresholds
  - AWS and application-level configuration
- Eliminates hardcoding and ensures consistency across the system.

### Entity Layer (`entity/`)
- Acts as the **contract layer** between pipeline stages.

**Config Entities (`config_entity.py`)**
- Define configuration classes for each pipeline stage.
- Create required directories and file paths using constants.
- Ensure components receive only required configuration.

**Artifact Entities (`artifact_entity.py`)**
- Define output objects for each pipeline stage.
- Store paths, metrics, and status flags.
- Enable explicit and traceable data flow between pipeline stages.

### Component Layer (`components/`)
- Contains the core logic for each pipeline stage.
- Each component:
  - Performs a single responsibility
  - Accepts a config object as input
  - Produces an artifact object as output
- Components include:
  - Data Ingestion
  - Data Validation
  - Data Transformation
  - Model Training
  - Model Evaluation
  - Model Pusher

### Pipeline Orchestration (`pipeline/`)
- Responsible for executing and coordinating pipeline stages.

**Training Pipeline**
- Initializes all pipeline configurations.
- Executes components sequentially.
- Passes artifacts between stages.
- Runs the full ML lifecycle from ingestion to model deployment.

**Prediction Pipeline**
- Loads the trained model from S3.
- Applies preprocessing and generates predictions.
- Used by FastAPI endpoints and the UI.
- Fully decoupled from the training pipeline.

### Application Layer (`app.py`)
- Exposes APIs for training and prediction.
- Connects the UI to the prediction pipeline.
- Acts as the user-facing interface of the ML system.



### 1ï¸âƒ£ Training Pipeline (`training_pipeline.py`)

Runs the **entire ML lifecycle**:

1. **Data Ingestion**
   - Fetches data from MongoDB
   - Stores raw data in feature store

2. **Data Validation**
   - Schema validation
   - Column checks
   - Data drift detection using Evidently

3. **Data Transformation**
   - Feature engineering (e.g. company age)
   - Encoding (OneHot, Ordinal)
   - Scaling & power transforms
   - Class imbalance handling (SMOTEENN)

4. **Model Training**
   - GridSearchCV over multiple models
   - Performance threshold enforcement

5. **Model Evaluation**
   - Compare new model vs existing production model
   - Accept only if performance improves

6. **Model Pusher**
   - Uploads accepted model to AWS S3 (model registry)

---

### 2ï¸âƒ£ Prediction Pipeline (`prediction_pipeline.py`)

Used during inference:

1. Accepts user input
2. Converts input to DataFrame
3. Loads model from S3
4. Applies preprocessing + prediction
5. Returns prediction result

---

## ðŸš€ How to Run the Project

### ðŸ”§ Prerequisites

- Python 3.12.12
- Docker
- AWS account
- MongoDB connection
- EC2 instance with IAM role

---

### â–¶ï¸ Local Development

#### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/jsuryanm/visa-approval-ml-system.git
```

#### 2ï¸âƒ£ Create virtual environment
```bash
conda create --name myenv python=3.12
conda activate myenv
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```


### 4ï¸âƒ£ Setup Environment Variables in .env file
```bash
MONGODB_CONNECTION_URL="your_mongodb_url"
AWS_ACCESS_KEY_ID="AWS_ACCESS_KEY_ID"
AWS_SECRET_ACCESS_KEY_ID="AWS_SECRET_ACCESS_KEY_ID"
```

---

### Setup for Cloud Deployment with EC-2 and GitHub Actions CI/CD

#### 1. Login to AWS console.

#### 2. Create IAM user for deployment

	#with specific access

	1. EC2 access : It is virtual machine

	2. ECR: Elastic Container registry to save your docker image in aws


	#Description: About the deployment

	1. Build docker image of the source code

	2. Push your docker image to ECR

	3. Launch Your EC2 

	4. Pull Your image from ECR in EC2

	5. Lauch your docker image in EC2

	#Policy:

	1. AmazonEC2ContainerRegistryFullAccess

	2. AmazonEC2FullAccess

  	3. AmazonS3FullAccess

	
#### 3. Create ECR repo to store/save docker image
    - Save the URI: 315865595366.dkr.ecr.ap-southeast-1.amazonaws.com/visarepo

	
#### 4. Create EC2 machine (Ubuntu) 

#### 5. Open EC2 and Install docker in EC2 Machine:
	
	sudo apt-get update -y

	sudo apt-get upgrade
	
	#required

	curl -fsSL https://get.docker.com -o get-docker.sh

	sudo sh get-docker.sh

	sudo usermod -aG docker ubuntu

	newgrp docker
	
#### 6. Configure EC2 as self-hosted runner:
    setting -> actions -> runner -> new self hosted runner -> choose linux os -> then run command one by one


#### 7. Setup github secrets:

   - AWS_ACCESS_KEY_ID
   - AWS_SECRET_ACCESS_KEY
   - AWS_DEFAULT_REGION
   - ECR_REPO
   - MONGODB_CONNECTION_URL

---
### Future improvements to implement
- Scalable Inference with ALB
- Streamlit UI frontend deployment
- Model Tracking and Experiment Management with MLFlow








